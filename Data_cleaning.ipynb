{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32351d8d",
   "metadata": {},
   "source": [
    "###### lets first start with setting up the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61872145",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd13dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc9daf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb52e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aca690",
   "metadata": {},
   "source": [
    "loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6266d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "articles_df = pd.read_csv(\"../Dataset/HandM/articles.csv\",dtype={\"article_id\": \"str\"})\n",
    "articles_df.article_id = article_id_str_to_int(articles_df.article_id)\n",
    "\n",
    "customers_df = pd.read_csv(\"../Dataset/HandM/customers.csv\")\n",
    "customers_df['customer_id'] = customer_hex_id_to_int(customers_df['customer_id'])\n",
    "for col in ['FN', 'Active', 'age']:\n",
    "    customers_df[col].fillna(-1, inplace=True)\n",
    "    customers_df[col] = customers_df[col].astype('int8')\n",
    "\n",
    "\n",
    "sample_submission_df = pd.read_csv(\"../Dataset/HandM/sample_submission.csv\")\n",
    "\n",
    "transaction_train = pd.read_csv(\"../Dataset/HandM/transactions_train.csv\",dtype={\"article_id\": \"str\"})\n",
    "transaction_train['customer_id'] = customer_hex_id_to_int(transaction_train['customer_id'])\n",
    "transaction_train.t_dat = pd.to_datetime(transaction_train.t_dat, format='%Y-%m-%d')\n",
    "transaction_train.article_id = article_id_str_to_int(transaction_train.article_id)\n",
    "transaction_train['week'] = 104 - (transaction_train.t_dat.max() - transaction_train.t_dat).dt.days // 7\n",
    "\n",
    "transaction_train[\"t_dat\"] = pd.to_datetime(transaction_train[\"t_dat\"])\n",
    "#taking only the first year\n",
    "transaction_train = transaction_train[transaction_train[\"t_dat\"] > \"2019-09-01\"]\n",
    "\n",
    "transaction_train[\"week\"] = (transaction_train[\"t_dat\"].max() - transaction_train[\"t_dat\"]).dt.days // 7\n",
    "transaction_train[\"week\"].value_counts()\n",
    "\n",
    "transaction_train.week = transaction_train.week.astype('int8')\n",
    "transaction_train.sales_channel_id = transaction_train.sales_channel_id.astype('int8')\n",
    "transaction_train.price = transaction_train.price.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a2f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_USERS = customers_df['customer_id'].unique().tolist()\n",
    "\n",
    "ALL_ITEMS = articles_df['article_id'].unique().tolist()\n",
    "\n",
    "user_to_customer_map = {user_id : customer_id for user_id, customer_id in enumerate(ALL_USERS)}\n",
    "customer_to_user_map = {customer_id : user_id for user_id, customer_id in enumerate(ALL_USERS)}\n",
    "\n",
    "item_to_article_map = {item_id : article_id for item_id, article_id in enumerate(ALL_ITEMS)}\n",
    "article_to_item_map = {article_id : item_id for item_id, article_id in enumerate(ALL_ITEMS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346ac6ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers_df['user_id'] = customers_df['customer_id'].map(customer_to_user_map)\n",
    "customers_df['user_id'] = customers_df['user_id'].astype('int8')\n",
    "\n",
    "customers_df.club_member_status = Categorize().fit_transform(customers_df[['club_member_status']]).club_member_status\n",
    "customers_df.postal_code = Categorize().fit_transform(customers_df[['postal_code']]).postal_code\n",
    "customers_df.fashion_news_frequency = Categorize().fit_transform(customers_df[['fashion_news_frequency']]).fashion_news_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be28de60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in articles_df.columns:\n",
    "    if articles_df[col].dtype == 'object':\n",
    "        articles_df[col] = Categorize().fit_transform(articles_df[[col]])[col]\n",
    "\n",
    "for col in articles_df.columns:\n",
    "    if articles_df[col].dtype == 'int64':\n",
    "        articles_df[col] = articles_df[col].astype('int32')        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e9b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8f454",
   "metadata": {},
   "source": [
    "    Since we have relatively clean data. We can start transforming our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb458d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_dat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_channel_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total  Percent\n",
       "t_dat                 0      0.0\n",
       "customer_id           0      0.0\n",
       "article_id            0      0.0\n",
       "price                 0      0.0\n",
       "sales_channel_id      0      0.0\n",
       "week                  0      0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data(transaction_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4bb37",
   "metadata": {},
   "source": [
    "Dimension reduction since we will be taking the last year of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d38a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15637202 entries, 16151122 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   customer_id       uint64 \n",
      " 1   article_id        int32  \n",
      " 2   price             float32\n",
      " 3   sales_channel_id  int8   \n",
      " 4   week              int8   \n",
      "dtypes: float32(1), int32(1), int8(2), uint64(1)\n",
      "memory usage: 387.7 MB\n"
     ]
    }
   ],
   "source": [
    "transaction_train.drop(columns='t_dat').info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e93712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1371980 entries, 0 to 1371979\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count    Dtype \n",
      "---  ------                  --------------    ----- \n",
      " 0   customer_id             1371980 non-null  uint64\n",
      " 1   FN                      1371980 non-null  int8  \n",
      " 2   Active                  1371980 non-null  int8  \n",
      " 3   club_member_status      1371980 non-null  int8  \n",
      " 4   fashion_news_frequency  1371980 non-null  int8  \n",
      " 5   age                     1371980 non-null  int8  \n",
      " 6   postal_code             1371980 non-null  int32 \n",
      " 7   user_id                 1371980 non-null  int8  \n",
      "dtypes: int32(1), int8(6), uint64(1)\n",
      "memory usage: 23.6 MB\n"
     ]
    }
   ],
   "source": [
    "customers_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31afd13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id                      int32\n",
      "product_code                    int32\n",
      "prod_name                       int32\n",
      "product_type_no                 int32\n",
      "product_type_name               int16\n",
      "product_group_name               int8\n",
      "graphical_appearance_no         int32\n",
      "graphical_appearance_name        int8\n",
      "colour_group_code               int32\n",
      "colour_group_name                int8\n",
      "perceived_colour_value_id       int32\n",
      "perceived_colour_value_name      int8\n",
      "perceived_colour_master_id      int32\n",
      "perceived_colour_master_name     int8\n",
      "department_no                   int32\n",
      "department_name                 int16\n",
      "index_code                       int8\n",
      "index_name                       int8\n",
      "index_group_no                  int32\n",
      "index_group_name                 int8\n",
      "section_no                      int32\n",
      "section_name                     int8\n",
      "garment_group_no                int32\n",
      "garment_group_name               int8\n",
      "detail_desc                     int32\n",
      "dtype: object\n",
      "t_dat               datetime64[ns]\n",
      "customer_id                 uint64\n",
      "article_id                   int32\n",
      "price                      float32\n",
      "sales_channel_id              int8\n",
      "week                          int8\n",
      "dtype: object\n",
      "customer_id               uint64\n",
      "FN                          int8\n",
      "Active                      int8\n",
      "club_member_status          int8\n",
      "fashion_news_frequency      int8\n",
      "age                         int8\n",
      "postal_code                int32\n",
      "user_id                     int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(articles_df.dtypes)\n",
    "print(transaction_train.dtypes)\n",
    "print(customers_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec58309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# transaction_train.to_parquet('../Dataset/HandM/transaction_train.parquet')\n",
    "# customers_df.to_parquet('../Dataset/HandM/customers.parquet')\n",
    "# articles_df.to_parquet('../Dataset/HandM/articles.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed0822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # let's create a 5% sample of the entiriety of the data to speed up dev\n",
    "\n",
    "# sample = 0.05\n",
    "# customers_sample = customers.sample(frac=sample, replace=False)\n",
    "# customers_sample_ids = set(customers_sample['customer_id'])\n",
    "# transactions_sample = transactions[transactions[\"customer_id\"].isin(customers_sample_ids)]\n",
    "# articles_sample_ids = set(transactions_sample[\"article_id\"])\n",
    "# articles_sample = articles[articles[\"article_id\"].isin(articles_sample_ids)]\n",
    "\n",
    "# customers_sample.to_parquet(f'data/customers_sample_{sample}.parquet', index=False)\n",
    "# transactions_sample.to_parquet(f'data/transactions_train_sample_{sample}.parquet', index=False)\n",
    "# articles_sample.to_parquet(f'data/articles_train_sample_{sample}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114eb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42dd05a",
   "metadata": {},
   "source": [
    "##### create mapping from ids to incremental integers and viceversa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66bb8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# week is between 0 - 104 # roughly two years\n",
    "WEEK_HIST_MAX = 5\n",
    "def create_dataset(transaction_train, articles_df, week):\n",
    "    \n",
    "    #encode article_id and customer_id\n",
    "    transaction_train['item_id'] = transaction_train['article_id'].map(article_to_item_map)\n",
    "    transaction_train['user_id'] = transaction_train['customer_id'].map(customer_to_user_map)\n",
    "    \n",
    "    cols = ['article_id','prod_name','department_name']\n",
    "    articles = articles_df[cols]\n",
    "    articles['item_id'] = articles['article_id'].map(article_to_item_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    hist_df = transaction_train[(transaction_train[\"week\"] > week) & (transaction_train[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = transaction_train.merge(articles,how = 'left', left_on = 'item_id', right_on='item_id')\n",
    "    \n",
    "    hist_df = hist_df.groupby(\"user_id\").agg({\"item_id\": list, \"week\": list, \"prod_name\": list}).reset_index()\n",
    "    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n",
    "    \n",
    "    target_df = transaction_train[transaction_train[\"week\"] == week]\n",
    "    target_df = target_df.groupby(\"user_id\").agg({\"item_id\": list}).reset_index()\n",
    "    target_df.rename(columns={\"item_id\": \"target\"}, inplace=True)\n",
    "    target_df[\"week\"] = week\n",
    "    \n",
    "    return target_df.merge(hist_df, on=\"user_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6515229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19095\\AppData\\Local\\Temp/ipykernel_8204/2369803444.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  articles['item_id'] = articles['article_id'].map(article_to_item_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_weeks = [0]\n",
    "train_weeks = [1, 2, 3, 4]\n",
    "\n",
    "val_df = pd.concat([create_dataset(transaction_train, articles_df, w) for w in val_weeks]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8043230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19095\\AppData\\Local\\Temp/ipykernel_8204/2369803444.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  articles['item_id'] = articles['article_id'].map(article_to_item_map)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([create_dataset(transaction_train, articles_df, w) for w in train_weeks]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc84dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>week</th>\n",
       "      <th>item_id</th>\n",
       "      <th>week_history</th>\n",
       "      <th>prod_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[78503]</td>\n",
       "      <td>1</td>\n",
       "      <td>[59458, 1469, 1469, 60253, 60259, 93585, 91841...</td>\n",
       "      <td>[33, 33, 33, 24, 24, 22, 22, 22, 22, 22, 22, 2...</td>\n",
       "      <td>[5126, 894, 894, 394, 394, 21703, 452, 2303, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>[58295, 3091]</td>\n",
       "      <td>1</td>\n",
       "      <td>[57068, 75961, 58295, 3091]</td>\n",
       "      <td>[48, 48, 1, 1]</td>\n",
       "      <td>[129, 6167, 3330, 259]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>[61916]</td>\n",
       "      <td>1</td>\n",
       "      <td>[39379, 75146, 35495, 52962, 86442, 771, 40074...</td>\n",
       "      <td>[54, 54, 54, 54, 54, 54, 54, 52, 52, 52, 52, 5...</td>\n",
       "      <td>[3962, 24441, 33658, 8707, 2286, 975, 6609, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>[33868, 27905, 98606, 98606, 100228]</td>\n",
       "      <td>1</td>\n",
       "      <td>[86215, 1780, 96635, 61602, 97677, 76208, 9723...</td>\n",
       "      <td>[26, 26, 26, 12, 12, 7, 7, 7, 6, 6, 6, 2, 2, 2...</td>\n",
       "      <td>[28266, 249, 18750, 435, 5299, 14086, 7546, 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>[97666, 97667, 97666, 97667]</td>\n",
       "      <td>1</td>\n",
       "      <td>[33971, 69224, 80087, 33970, 62346, 62346, 708...</td>\n",
       "      <td>[51, 51, 51, 51, 47, 47, 47, 23, 23, 23, 23, 1...</td>\n",
       "      <td>[2358, 14565, 13390, 2358, 10395, 10395, 40631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300124</th>\n",
       "      <td>1371937</td>\n",
       "      <td>[59774]</td>\n",
       "      <td>4</td>\n",
       "      <td>[82437, 102944, 59774, 67261, 70640]</td>\n",
       "      <td>[20, 12, 4, 0, 0]</td>\n",
       "      <td>[16786, 1756, 1086, 884, 7488]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300125</th>\n",
       "      <td>1371949</td>\n",
       "      <td>[101374, 101368]</td>\n",
       "      <td>4</td>\n",
       "      <td>[50455, 26106, 46399, 10745, 80900, 20089, 610...</td>\n",
       "      <td>[54, 51, 51, 51, 51, 51, 51, 51, 50, 46, 46, 4...</td>\n",
       "      <td>[15498, 5580, 70, 20677, 10291, 108, 1294, 206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300126</th>\n",
       "      <td>1371960</td>\n",
       "      <td>[54341, 105185]</td>\n",
       "      <td>4</td>\n",
       "      <td>[83093, 83093, 85501, 85501, 87812, 89382, 209...</td>\n",
       "      <td>[50, 50, 50, 50, 39, 20, 20, 20, 20, 20, 20, 2...</td>\n",
       "      <td>[11053, 11053, 27945, 27945, 9609, 24828, 307,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300127</th>\n",
       "      <td>1371963</td>\n",
       "      <td>[96812, 96272]</td>\n",
       "      <td>4</td>\n",
       "      <td>[72045, 64220, 90556, 90556, 67584, 93090, 681...</td>\n",
       "      <td>[52, 52, 36, 36, 36, 36, 36, 12, 11, 11, 10, 1...</td>\n",
       "      <td>[14704, 2002, 25534, 25534, 11349, 30282, 6125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300128</th>\n",
       "      <td>1371969</td>\n",
       "      <td>[82965, 91131]</td>\n",
       "      <td>4</td>\n",
       "      <td>[73421, 82808, 74509, 81946, 69693, 82398, 812...</td>\n",
       "      <td>[50, 44, 34, 31, 31, 31, 31, 31, 31, 31, 17, 1...</td>\n",
       "      <td>[13971, 16907, 2351, 30914, 2305, 16740, 5534,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300129 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                                target  week  \\\n",
       "0             2                               [78503]     1   \n",
       "1             6                         [58295, 3091]     1   \n",
       "2            38                               [61916]     1   \n",
       "3            86  [33868, 27905, 98606, 98606, 100228]     1   \n",
       "4            90          [97666, 97667, 97666, 97667]     1   \n",
       "...         ...                                   ...   ...   \n",
       "300124  1371937                               [59774]     4   \n",
       "300125  1371949                      [101374, 101368]     4   \n",
       "300126  1371960                       [54341, 105185]     4   \n",
       "300127  1371963                        [96812, 96272]     4   \n",
       "300128  1371969                        [82965, 91131]     4   \n",
       "\n",
       "                                                  item_id  \\\n",
       "0       [59458, 1469, 1469, 60253, 60259, 93585, 91841...   \n",
       "1                             [57068, 75961, 58295, 3091]   \n",
       "2       [39379, 75146, 35495, 52962, 86442, 771, 40074...   \n",
       "3       [86215, 1780, 96635, 61602, 97677, 76208, 9723...   \n",
       "4       [33971, 69224, 80087, 33970, 62346, 62346, 708...   \n",
       "...                                                   ...   \n",
       "300124               [82437, 102944, 59774, 67261, 70640]   \n",
       "300125  [50455, 26106, 46399, 10745, 80900, 20089, 610...   \n",
       "300126  [83093, 83093, 85501, 85501, 87812, 89382, 209...   \n",
       "300127  [72045, 64220, 90556, 90556, 67584, 93090, 681...   \n",
       "300128  [73421, 82808, 74509, 81946, 69693, 82398, 812...   \n",
       "\n",
       "                                             week_history  \\\n",
       "0       [33, 33, 33, 24, 24, 22, 22, 22, 22, 22, 22, 2...   \n",
       "1                                          [48, 48, 1, 1]   \n",
       "2       [54, 54, 54, 54, 54, 54, 54, 52, 52, 52, 52, 5...   \n",
       "3       [26, 26, 26, 12, 12, 7, 7, 7, 6, 6, 6, 2, 2, 2...   \n",
       "4       [51, 51, 51, 51, 47, 47, 47, 23, 23, 23, 23, 1...   \n",
       "...                                                   ...   \n",
       "300124                                  [20, 12, 4, 0, 0]   \n",
       "300125  [54, 51, 51, 51, 51, 51, 51, 51, 50, 46, 46, 4...   \n",
       "300126  [50, 50, 50, 50, 39, 20, 20, 20, 20, 20, 20, 2...   \n",
       "300127  [52, 52, 36, 36, 36, 36, 36, 12, 11, 11, 10, 1...   \n",
       "300128  [50, 44, 34, 31, 31, 31, 31, 31, 31, 31, 17, 1...   \n",
       "\n",
       "                                                prod_name  \n",
       "0       [5126, 894, 894, 394, 394, 21703, 452, 2303, 2...  \n",
       "1                                  [129, 6167, 3330, 259]  \n",
       "2       [3962, 24441, 33658, 8707, 2286, 975, 6609, 13...  \n",
       "3       [28266, 249, 18750, 435, 5299, 14086, 7546, 75...  \n",
       "4       [2358, 14565, 13390, 2358, 10395, 10395, 40631...  \n",
       "...                                                   ...  \n",
       "300124                     [16786, 1756, 1086, 884, 7488]  \n",
       "300125  [15498, 5580, 70, 20677, 10291, 108, 1294, 206...  \n",
       "300126  [11053, 11053, 27945, 27945, 9609, 24828, 307,...  \n",
       "300127  [14704, 2002, 25534, 25534, 11349, 30282, 6125...  \n",
       "300128  [13971, 16907, 2351, 30914, 2305, 16740, 5534,...  \n",
       "\n",
       "[300129 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d985c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d73ddbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_ids = np.concatenate([[\"placeholder\"], np.unique(articles_df[\"article_id\"].values)])\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_len = seq_len\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        if self.is_test:\n",
    "            target = torch.zeros(2).float() # buy/not buy\n",
    "        else: \n",
    "            target = torch.zeros(len(article_ids)).float()\n",
    "            for t in row.target:\n",
    "                target[t] = 1.0\n",
    "            \n",
    "        item_hist = torch.zeros(self.seq_len).long()\n",
    "        week_hist = torch.ones(self.seq_len).float()\n",
    "        prod_name_hist = torch.zeros(self.seq_len).long()\n",
    "        \n",
    "        # encoding each data point into tensors\n",
    "        \n",
    "        if isinstance(row.item_id, list):\n",
    "            if len(row.item_id) >= self.seq_len:\n",
    "                item_hist = torch.LongTensor(row.item_id[-self.seq_len:])\n",
    "                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/WEEK_HIST_MAX/2\n",
    "                prod_name_hist = torch.LongTensor(row.prod_name[-self.seq_len:])\n",
    "            else:\n",
    "                item_hist[-len(row.item_id):] = torch.LongTensor(row.item_id)\n",
    "                week_hist[-len(row.item_id):] = (torch.LongTensor(row.week_history) - row.week)/WEEK_HIST_MAX/2\n",
    "                prod_name_hist[-len(row.item_id):] = torch.LongTensor(row.prod_name)\n",
    "                \n",
    "        return item_hist, week_hist, prod_name_hist, target\n",
    "    \n",
    "# seq_len is how long the tensor will be.  t\n",
    "\n",
    "# why if it is test, we can have torch \n",
    "# leng of 2 while the other is seq_len \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91198fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,  86215,\n",
       "           1780,  96635,  61602,  97677,  76208,  97234,  97234, 104805, 104209,\n",
       "         100630, 103583, 102472, 102710,  33868,  27905,  98606,  98606, 100228,\n",
       "          87371]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 2.6000,\n",
       "         2.6000, 2.6000, 1.2000, 1.2000, 0.7000, 0.7000, 0.7000, 0.6000, 0.6000,\n",
       "         0.6000, 0.2000, 0.2000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.0000]),\n",
       " tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0, 28266,   249, 18750,   435,  5299, 14086,\n",
       "          7546,  7546, 25211,  6649, 17606, 25711, 21061,  9649,  1074,   867,\n",
       "         27346, 27346, 30684, 11517]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMDataset(val_df, 64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9145fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = HMDataset(val_df, 64)\n",
    "train = HMDataset(train_df, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c9872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch < 1:\n",
    "        lr = 5e-5\n",
    "    elif epoch < 6:\n",
    "        lr = 1e-3\n",
    "    elif epoch < 9:\n",
    "        lr = 1e-4\n",
    "    else:\n",
    "        lr = 1e-5\n",
    "\n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = lr\n",
    "    return lr\n",
    "    \n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-08)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d06e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HMModel(nn.Module):\n",
    "    def __init__(self, article_shape):\n",
    "        super(HMModel, self).__init__()\n",
    "        \n",
    "        self.article_emb = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n",
    "#         self.prod_emb = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n",
    "        \n",
    "        self.article_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n",
    "        self.top = nn.Sequential(nn.Conv1d(4, 32, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(8, 1, kernel_size=1))\n",
    "#         why is the out put here 1?\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        article_hist, week_hist = inputs[0], inputs[1]\n",
    "        \n",
    "        print(\"ARTICLE HISTORY\")\n",
    "        print(article_hist)\n",
    "        print(\"article_hist shape\")\n",
    "        print(article_hist.shape)\n",
    "        \n",
    "        \n",
    "        prod_hist = inputs[2]\n",
    "        \n",
    "        \n",
    "        print(\"prod_hist HISTORY\")\n",
    "        print(prod_hist)\n",
    "        print(\"prod_hist shape\")\n",
    "        print(prod_hist.shape)\n",
    "        \n",
    "        x = self.article_emb(article_hist)\n",
    "        x = F.normalize(x, dim=2)\n",
    "        x = x@F.normalize(self.article_emb.weight).T\n",
    "        x, indices = x.max(axis=1)\n",
    "        x = x.clamp(1e-3, 0.999)\n",
    "        x = -torch.log(1/x - 1)\n",
    "        \n",
    "        \n",
    "#         x1 = self.prod_emb(prod_hist)\n",
    "#         x1 = F.normalize(x1, dim=2)\n",
    "#         x1 = x@F.normalize(self.prod_emb.weight).T\n",
    "#         x1, indices = x1.max(axis=1)\n",
    "#         x1 = x.clamp(1e-3, 0.999)\n",
    "#         x1 = -torch.log(1/x1 - 1)\n",
    "        \n",
    "        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n",
    "        max_week = max_week.mean(axis=1).unsqueeze(1)\n",
    "        \n",
    "        max_prod = prod_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, prod_hist.shape[1], 1))\n",
    "        max_prod = max_prod.mean(axis=1).unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat([x.unsqueeze(1), max_week, max_prod,\n",
    "                       self.article_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n",
    "\n",
    "        \n",
    "        x = self.top(x).squeeze(1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = HMModel((len(transaction_train.item_id.unique()), 512))\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca28397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def calc_map(topk_preds, target_array, k=12):\n",
    "    metric = []\n",
    "    tp, fp = 0, 0\n",
    "    \n",
    "    for pred in topk_preds:\n",
    "        if target_array[pred]:\n",
    "            tp += 1\n",
    "            metric.append(tp/(tp + fp))\n",
    "        else:\n",
    "            fp += 1\n",
    "            \n",
    "    return np.sum(metric) / min(k, target_array.sum())\n",
    "\n",
    "def read_data(data):\n",
    "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
    "\n",
    "\n",
    "def validate(model, val_loader, k=12):\n",
    "    model.eval()\n",
    "    \n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "    \n",
    "    maps = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            _, indices = torch.topk(logits, k, dim=1)\n",
    "\n",
    "            indices = indices.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "\n",
    "            for i in range(indices.shape[0]):\n",
    "                maps.append(calc_map(indices[i], target[i]))\n",
    "        \n",
    "    \n",
    "    return np.mean(maps)\n",
    "\n",
    "SEQ_LEN = 16\n",
    "\n",
    "BS = 256\n",
    "NW = 8\n",
    "\n",
    "val_dataset = HMDataset(val_df, SEQ_LEN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1172 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def dice_loss(y_pred, y_true):\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    intersect = (y_true*y_pred).sum(axis=1)\n",
    "    \n",
    "    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        \n",
    "        lr = adjust_lr(optimizer, e)\n",
    "        \n",
    "        loss_list = []\n",
    "\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, target) + dice_loss(logits, target)\n",
    "            \n",
    "            \n",
    "            #loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            #optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            \n",
    "            avg_loss = np.round(100*np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n",
    "            \n",
    "        val_map = validate(model, val_loader)\n",
    "\n",
    "        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\\nValidation MAP: {val_map}\\n\"\n",
    "            \n",
    "        print(log_text)\n",
    "        \n",
    "        #logfile = open(f\"models/{MODEL_NAME}_{SEED}.txt\", 'a')\n",
    "        #logfile.write(log_text)\n",
    "        #logfile.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_NAME = \"exp001\"\n",
    "SEED = 0\n",
    "\n",
    "train_dataset = HMDataset(train_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=True)\n",
    "\n",
    "# model = train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97792f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a57371",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.prod_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d3896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
